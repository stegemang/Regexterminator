{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexterminator script\n",
    "\n",
    "### To do's:\n",
    "- [ ] scrape SO and/or github for common regex solutions\n",
    "- [ ] scrape for common regex problems/data sets to train on\n",
    "- [ ] engineer features\n",
    "   - [ ] checking out text aligning tools from genetics?\n",
    "   - [ ] pre-trained NN layer for parts of text/speech?\n",
    "- [ ] make model work\n",
    "- [ ] make flask app/website\n",
    "- [ ] take in multiple inputs examples\n",
    "- [ ] take in feedback\n",
    "- [ ] run on a cloud server, train massive dataset\n",
    "- [ ] Use reinforcement somehow? reward short answers?\n",
    "\n",
    "\n",
    "### common naming mix-ups\n",
    "start and original, are the sentences/stings in the raw data\n",
    "\n",
    "end and replacement are the sentences/string after the regex is applied\n",
    "\n",
    "sentences are the input/example/sample to emulate what the user might enter. They do not need to be sentences with words, can be numbers, jibberisht, weird strings of characters etc. Hopefully something from a regular language though...\n",
    "\n",
    "## fun times~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole #Initiate console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "import re\n",
    "import pickle # exporting structures for external saves and quick read back into python\n",
    "import string # for counting type of characters in strings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../\"\n",
    "path = \"/Users/gregorystegeman/Documents/Regexterminator/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: use stack overflow API to find top regex queries\\nMaybe searh for str.replace in gitHub\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/tagged/regex\n",
    "'''\n",
    "TODO: use stack overflow API to find top regex queries\n",
    "Maybe searh for str.replace in gitHub\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super simple set for easy following though pipeline here:\n",
    "\n",
    "sample_regex_patterns = [r'\\d+',\n",
    "                         r'[a-z]',\n",
    "                         r'[A-Z]',\n",
    "                         r'\\s',\n",
    "                         r\"[^A-Za-z0-9]\",\n",
    "                         r'[0-9]',\n",
    "                        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_regex_patterns = [\n",
    "    r'\\d+',\n",
    "    r'[a-z]',\n",
    "    r'[A-Z]',\n",
    "    r'\\s',\n",
    "    r'[^A-Za-z0-9]',\n",
    "    r'[0-9]',\n",
    "    r'http\\S+',\n",
    "    r'(.*);(.*)',\n",
    "    r'^(\\d{3})-(\\d{3})-(\\d{4})$',\n",
    "    r'^(\\d{3})\\D+(\\d{3})\\D+(\\d{4})\\D+(\\d+)$',\n",
    "    r'([\\w0-9_]+)',\n",
    "    r'^.+',\n",
    "    r'([^:：]+)[:：]?\\s*',\n",
    "    r'\\d+/\\d+/\\d+',\n",
    "    r'\\.[0]*',\n",
    "    r'[,;_]',\n",
    "    r'[\\w\\d]{1,20}[\\w]{1,20}[\\w]{1,5}',\n",
    "    r'(.+) \\1',\n",
    "    r'http\\S+\\s',\n",
    "    r'RT|cc',\n",
    "    r'[^A-Za-z0-9]+',\n",
    "    r'\\w',\n",
    "    r'\\n|\\r',\n",
    "    r'^([A-Za-z]\\d[A-Za-z][-]?\\d[A-Za-z]\\d)',\n",
    "    r'[:]{1}[-~+o]?[)&gt;]+',\n",
    "    r'(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)',\n",
    "    r'<[^>]*>',\n",
    "    r'\\xA9',\n",
    "    r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)',\n",
    "    r'@\\S+',\n",
    "    r'#\\S+',\n",
    "    r'^\\d+',\n",
    "    r'^[a-z]',\n",
    "    r'^[A-Z]',\n",
    "    r'^\\s',\n",
    "    r\"^[^A-Za-z0-9]\",\n",
    "    r'^[0-9]',\n",
    "    \n",
    "                        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary to reference regex pattern classes\n",
    "regex_dict = {}\n",
    "for i in sample_regex_patterns:\n",
    "    regex_dict['c_'+ str(sample_regex_patterns.index(i))] = i\n",
    "    \n",
    "#Save regex_dict in pickle; move to flask app later\n",
    "pickle.dump(regex_dict, open('regex_dict.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape/acquire sentences from: \n",
    "- [ ] twitter\n",
    "- [ ] wikipedia\n",
    "- [ ] yelp reviews (Ming He)\n",
    "- [ ] steam game reviews\n",
    "- [ ] copy from other fellows\n",
    "- [ ] Excel help forums\n",
    "- [ ] amazon reviews (Patrick Lestrange)\n",
    "- [ ] Medium (Kim S)\n",
    "- [ ] StackOverFlow comment and questions\n",
    "- [ ] BGG (Pam M)\n",
    "\n",
    "Stuff to include\n",
    "- numbers\n",
    "- phone numbers\n",
    "- postal codes\n",
    "- email address\n",
    "- code segments\n",
    "- dates of different formats\n",
    "- IP addresses\n",
    "- addresses\n",
    "- multi line?\n",
    "- non standard characters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller test dataset\n",
    "start_sentences = pd.read_csv(path + 'garbage_data.csv')\n",
    "\n",
    "start_sentences = start_sentences[\"garbage data\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloaded random dataset from https://www.figure-eight.com/data-for-everyone/\n",
    "df = pd.read_csv(path + '1377191648_sentiment_nuclear_power.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Only really care about tweet_text column\n",
    "start_sentences = start_sentences + df.tweet_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in 100 sample tweets from twitter100k dataset:\n",
    "tweetlines = open(path + \"tweet_samples.txt\").read().splitlines()\n",
    "\n",
    "start_sentences = start_sentences + tweetlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMS messages from:\n",
    "# https://github.com/kite1988/nus-sms-corpus\n",
    "import json\n",
    "with open(path + \"smsCorpus_en_2015.03.09_all.json\") as f:\n",
    "    sms_data = json.load(f)\n",
    "\n",
    "sms = sms_data['smsCorpus']\n",
    "sms = sms.get('message')\n",
    "\n",
    "sms_texts = []\n",
    "for message in sms:\n",
    "    sms_texts.append(message['text']['$'])\n",
    "\n",
    "# Maybe random later, for now keep it the same:    \n",
    "sms_subset = sms_texts[0:1000]\n",
    "\n",
    "start_sentences = start_sentences + sms_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake number data and then mess it up\n",
    "# phone numbers\n",
    "# zipcodes\n",
    "# prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to sentences a sentence that contains all alphanumeric characters that we ever expect to encounter\n",
    "\n",
    "start_sentences.append('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuv')\n",
    "start_sentences.append('`1234567890-=~!@#$%^&*()_+[]\\{}|;\\':\\\",./<>?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force all things in list to be strings\n",
    "\n",
    "start_sentences = [str(i) for i in start_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Feature Engineering\n",
    "\n",
    "## 2.0 Generate training data with unmodified/start sentence and a processed replacement/end sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a new data frame that captures both the regex and also the \n",
    "sentence transformation\n",
    "'''\n",
    "start_regexed_end = pd.DataFrame() #Initialize empty data frame\n",
    "\n",
    "for regex in sample_regex_patterns:\n",
    "    for sentence in start_sentences:\n",
    "        r = re.compile(regex)\n",
    "        d = {\n",
    "            'sentence': sentence,\n",
    "            'regex': regex,\n",
    "            'replacement': re.sub(r,'',sentence)\n",
    "        }\n",
    "        \n",
    "        start_regexed_end = start_regexed_end.append(d, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/pickle it, that takes a long time to run:\n",
    "\n",
    "pickle.dump(start_regexed_end, open('start_regexed_end.pickle', 'wb'))\n",
    "\n",
    "start_regexed_end = pickle.load(open('start_regexed_end.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>replacement</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#\\S+</th>\n",
       "      <td>1221</td>\n",
       "      <td>1221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(.*);(.*)</th>\n",
       "      <td>1296</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(.+) \\1</th>\n",
       "      <td>975</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(\\:\\w+\\:|\\&lt;[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)</th>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;[^&gt;]*&gt;</th>\n",
       "      <td>1325</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@\\S+</th>\n",
       "      <td>1115</td>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT|cc</th>\n",
       "      <td>1197</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[,;_]</th>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0-9]</th>\n",
       "      <td>971</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[:]{1}[-~+o]?[)&amp;gt;]+</th>\n",
       "      <td>1313</td>\n",
       "      <td>1313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[A-Z]</th>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[\\w\\d]{1,20}[\\w]{1,20}[\\w]{1,5}</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[^A-Za-z0-9]</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[^A-Za-z0-9]+</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[a-z]</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\.[0]*</th>\n",
       "      <td>361</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\d+</th>\n",
       "      <td>971</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\d+/\\d+/\\d+</th>\n",
       "      <td>1326</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n|\\r</th>\n",
       "      <td>1326</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\s</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\xA9</th>\n",
       "      <td>1326</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^([A-Za-z]\\d[A-Za-z][-]?\\d[A-Za-z]\\d)</th>\n",
       "      <td>1325</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^(\\d{3})-(\\d{3})-(\\d{4})$</th>\n",
       "      <td>1326</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^(\\d{3})\\D+(\\d{3})\\D+(\\d{4})\\D+(\\d+)$</th>\n",
       "      <td>1326</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^[0-9]</th>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^[A-Z]</th>\n",
       "      <td>377</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^[^A-Za-z0-9]</th>\n",
       "      <td>1216</td>\n",
       "      <td>1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^[a-z]</th>\n",
       "      <td>1077</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^\\d+</th>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^\\s</th>\n",
       "      <td>1325</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http\\S+</th>\n",
       "      <td>1323</td>\n",
       "      <td>1323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http\\S+\\s</th>\n",
       "      <td>1325</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    replacement  sentence\n",
       "regex                                                                    \n",
       "#\\S+                                                       1221      1221\n",
       "(.*);(.*)                                                  1296      1296\n",
       "(.+) \\1                                                     975       975\n",
       "(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)              43        43\n",
       "(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;...         1215      1215\n",
       "<[^>]*>                                                    1325      1325\n",
       "@\\S+                                                       1115      1115\n",
       "RT|cc                                                      1197      1197\n",
       "[,;_]                                                      1127      1127\n",
       "[0-9]                                                       971       971\n",
       "[:]{1}[-~+o]?[)&gt;]+                                      1313      1313\n",
       "[A-Z]                                                       205       205\n",
       "[\\w\\d]{1,20}[\\w]{1,20}[\\w]{1,5}                              27        27\n",
       "[^A-Za-z0-9]                                                 13        13\n",
       "[^A-Za-z0-9]+                                                13        13\n",
       "[a-z]                                                         8         8\n",
       "\\.[0]*                                                      361       361\n",
       "\\d+                                                         971       971\n",
       "\\d+/\\d+/\\d+                                                1326      1326\n",
       "\\n|\\r                                                      1326      1326\n",
       "\\s                                                           53        53\n",
       "\\xA9                                                       1326      1326\n",
       "^([A-Za-z]\\d[A-Za-z][-]?\\d[A-Za-z]\\d)                      1325      1325\n",
       "^(\\d{3})-(\\d{3})-(\\d{4})$                                  1326      1326\n",
       "^(\\d{3})\\D+(\\d{3})\\D+(\\d{4})\\D+(\\d+)$                      1326      1326\n",
       "^[0-9]                                                     1308      1308\n",
       "^[A-Z]                                                      377       377\n",
       "^[^A-Za-z0-9]                                              1216      1216\n",
       "^[a-z]                                                     1077      1077\n",
       "^\\d+                                                       1308      1308\n",
       "^\\s                                                        1325      1325\n",
       "http\\S+                                                    1323      1323\n",
       "http\\S+\\s                                                  1325      1325"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much was changed/affected by regex's?\n",
    "\n",
    "'''\n",
    "1. Find if difference between start_regexed_end.sentence & start_regexed_end.replacement\n",
    "2. Group by 'regex'\n",
    "'''\n",
    "\n",
    "#print(start_regexed_end.groupby(['regex']).sum(start_regexed_end['sentence'] != start_regexed_end['replacement']))      \n",
    "start_regexed_end[start_regexed_end.sentence == start_regexed_end.replacement].groupby('regex').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also wondering which regex give the same result from each processing.\n",
    "# Which are redundant or overlapping all the time or part of the time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regex</th>\n",
       "      <th>replacement</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>blah blah! BLAH!!!</td>\n",
       "      <td>blah blah! BLAH!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>hjhjhj</td>\n",
       "      <td>42hj43hj34hj43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>..gf;gfsd;gg;</td>\n",
       "      <td>4.243.gf;gfsd;3g3g;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>fdsa.l fef. rlfkk</td>\n",
       "      <td>fdsa.2l fe3f. rlfkk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>derp derp doop.!!!</td>\n",
       "      <td>derp derp doop.!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  regex         replacement             sentence\n",
       "0   \\d+  blah blah! BLAH!!!   blah blah! BLAH!!!\n",
       "1   \\d+              hjhjhj       42hj43hj34hj43\n",
       "2   \\d+       ..gf;gfsd;gg;  4.243.gf;gfsd;3g3g;\n",
       "3   \\d+   fdsa.l fef. rlfkk  fdsa.2l fe3f. rlfkk\n",
       "4   \\d+  derp derp doop.!!!   derp derp doop.!!!"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_regexed_end.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Find differences between `start` and `end` sentence inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charCounts(column, colname):\n",
    "    \"\"\"\n",
    "    Counts the different type of characters in a string\n",
    "    \n",
    "    @param column: to apply lambda function to\n",
    "    @param colname: what the unique columns should be named\n",
    "    \n",
    "    \"\"\"\n",
    "    # This is really ugly, something I could do in R very easily:\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "    out = column.apply(\n",
    "    lambda s: pd.Series(\n",
    "        {colname+'punct': count(s,set(string.punctuation)),\n",
    "         colname+'letters': count(s,set(string.ascii_letters)),\n",
    "         colname+'digits': count(s,set(string.digits)),\n",
    "         colname+'lower': count(s,set(string.ascii_lowercase)),\n",
    "         colname+'upper': count(s,set(string.ascii_uppercase)),\n",
    "         colname+'whitespace': count(s,set(string.whitespace)),\n",
    "         colname+'words': len(s.split()),\n",
    "        }))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_counts = charCounts(start_regexed_end['sentence'], 'start_n_')\n",
    "end_counts = charCounts(start_regexed_end['replacement'], 'end_n_')\n",
    "\n",
    "type_counts = pd.concat([start_regexed_end, start_counts, end_counts], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regex</th>\n",
       "      <th>replacement</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_n_punct</th>\n",
       "      <th>start_n_letters</th>\n",
       "      <th>start_n_digits</th>\n",
       "      <th>start_n_lower</th>\n",
       "      <th>start_n_upper</th>\n",
       "      <th>start_n_whitespace</th>\n",
       "      <th>start_n_words</th>\n",
       "      <th>end_n_punct</th>\n",
       "      <th>end_n_letters</th>\n",
       "      <th>end_n_digits</th>\n",
       "      <th>end_n_lower</th>\n",
       "      <th>end_n_upper</th>\n",
       "      <th>end_n_whitespace</th>\n",
       "      <th>end_n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>blah blah! BLAH!!!</td>\n",
       "      <td>blah blah! BLAH!!!</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>hjhjhj</td>\n",
       "      <td>42hj43hj34hj43</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>..gf;gfsd;gg;</td>\n",
       "      <td>4.243.gf;gfsd;3g3g;</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>fdsa.l fef. rlfkk</td>\n",
       "      <td>fdsa.2l fe3f. rlfkk</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\d+</td>\n",
       "      <td>derp derp doop.!!!</td>\n",
       "      <td>derp derp doop.!!!</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  regex         replacement             sentence  start_n_punct  \\\n",
       "0   \\d+  blah blah! BLAH!!!   blah blah! BLAH!!!              4   \n",
       "1   \\d+              hjhjhj       42hj43hj34hj43              0   \n",
       "2   \\d+       ..gf;gfsd;gg;  4.243.gf;gfsd;3g3g;              5   \n",
       "3   \\d+   fdsa.l fef. rlfkk  fdsa.2l fe3f. rlfkk              2   \n",
       "4   \\d+  derp derp doop.!!!   derp derp doop.!!!              4   \n",
       "\n",
       "   start_n_letters  start_n_digits  start_n_lower  start_n_upper  \\\n",
       "0               12               0              8              4   \n",
       "1                6               8              6              0   \n",
       "2                8               6              8              0   \n",
       "3               13               2             13              0   \n",
       "4               12               0             12              0   \n",
       "\n",
       "   start_n_whitespace  start_n_words  end_n_punct  end_n_letters  \\\n",
       "0                   2              3            4             12   \n",
       "1                   0              1            0              6   \n",
       "2                   0              1            5              8   \n",
       "3                   2              3            2             13   \n",
       "4                   2              3            4             12   \n",
       "\n",
       "   end_n_digits  end_n_lower  end_n_upper  end_n_whitespace  end_n_words  \n",
       "0             0            8            4                 2            3  \n",
       "1             0            6            0                 0            1  \n",
       "2             0            8            0                 0            1  \n",
       "3             0           13            0                 2            3  \n",
       "4             0           12            0                 2            3  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering\n",
    "\n",
    "sequences of similar character types (letter, number, white space and punctuation)\n",
    "\n",
    "beginning and ends of sentences\n",
    "\n",
    "word boundaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def endChecks(dataf, start_col, end_col, regex = False):\n",
    "    \"\"\"\n",
    "    Checks if end words and characters are the same between two strings (from columns)\n",
    "    \n",
    "    @param dataf: pandas dataframe.\n",
    "    @param start_col: initial string\n",
    "    @param end_col: modified string to compare to\n",
    "    @param regex: include regex in new table for indexing.\n",
    "    \n",
    "    @return: returns dataframe with start column and regex for indexing, and four new columns of features\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame()\n",
    "    for index, row in dataf.iterrows():\n",
    "        tmp = {\n",
    "            start_col: row[start_col],\n",
    "            'regex': row['regex'] if regex else '',\n",
    "            'first_word_same': row[start_col].split()[0] == row[end_col].split()[0] if len(row[end_col].split())>0 else False,\n",
    "            'first_char_same': row[start_col][0] == row[end_col][0] if len(row[end_col].split())>0 else False,\n",
    "            'last_word_same': row[start_col].split()[-1] == row[end_col].split()[-1] if len(row[end_col].split())>0 else False,\n",
    "            'last_char_same': row[start_col][-1] == row[end_col][-1] if len(row[end_col].split())>0 else False,\n",
    "        }\n",
    "        out = out.append(tmp, ignore_index=True)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ends = endChecks(start_regexed_end, 'sentence','replacement', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more features?\n",
    "\n",
    "position of difference\n",
    "\n",
    "difference at beginning or end?\n",
    "\n",
    "aligned difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Test/Train data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature Variables (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features I want to work with and drop the label stuff\n",
    "\n",
    "#list(dummies.columns.values)\n",
    "list(type_counts.columns.values)\n",
    "\n",
    "features_combo = pd.merge(type_counts, string_ends, on=[\"sentence\",\"regex\"])\n",
    "\n",
    "list(features_combo.columns.values)\n",
    "\n",
    "X = features_combo.drop(['sentence', 'regex', 'replacement'], axis=1).values #Remove everything but the feature variables\n",
    "X = features_combo.drop(['sentence', 'regex', 'replacement'], axis=1) #Remove everything but the feature variables\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regex</th>\n",
       "      <th>replacement</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_n_punct</th>\n",
       "      <th>start_n_letters</th>\n",
       "      <th>start_n_digits</th>\n",
       "      <th>start_n_lower</th>\n",
       "      <th>start_n_upper</th>\n",
       "      <th>start_n_whitespace</th>\n",
       "      <th>start_n_words</th>\n",
       "      <th>...</th>\n",
       "      <th>end_n_letters</th>\n",
       "      <th>end_n_digits</th>\n",
       "      <th>end_n_lower</th>\n",
       "      <th>end_n_upper</th>\n",
       "      <th>end_n_whitespace</th>\n",
       "      <th>end_n_words</th>\n",
       "      <th>first_char_same</th>\n",
       "      <th>first_word_same</th>\n",
       "      <th>last_char_same</th>\n",
       "      <th>last_word_same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [regex, replacement, sentence, start_n_punct, start_n_letters, start_n_digits, start_n_lower, start_n_upper, start_n_whitespace, start_n_words, end_n_punct, end_n_letters, end_n_digits, end_n_lower, end_n_upper, end_n_whitespace, end_n_words, first_char_same, first_word_same, last_char_same, last_word_same]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Target Variable (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert regex labels to numeric value depending on where it occurs in the sample_regex_patterns list\n",
    "'''\n",
    "y = ['c_'+str(sample_regex_patterns.index(i)) for i in features_combo.regex]\n",
    "\n",
    "#Find the value (features_combo.regex) for the key in regex_dict\n",
    "def findRegexClass(dict_to_search, dict_value):\n",
    "    \"\"\"\n",
    "    Function to find the key \n",
    "    \n",
    "    @param dict_to_search: The dict to search for the key for the given value\n",
    "    @param dict_value: The value to find the corresponding key for\n",
    "    \n",
    "    @return: the key\n",
    "    \"\"\"\n",
    "    return [key for key, value in dict_to_search.items() if dict_value == value][0]\n",
    "\n",
    "# For each regex in dummies.regex, find the corresponding class/key to form the y-label\n",
    "y = [findRegexClass(regex_dict, i) for i in features_combo.regex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Test/Train data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import xgboost as xgb\\ndtrain = xgb.DMatrix(X_train, label=y_train)\\ndtest = xgb.DMatrix(X_test, label=y_test)\\n\\nparam = {\\n    'max_depth': 3,  # the maximum depth of each tree\\n    'eta': 0.3,  # the training step for each iteration\\n    'silent': 1,  # logging mode - quiet\\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\\n    'num_class': 3}  # the number of classes that exist in this datset\\nnum_round = 20  # the number of training iterations\\n\\nbst = xgb.train(param, dtrain, num_round)\""
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.kdnuggets.com/2017/03/simple-xgboost-tutorial-iris-dataset.html\n",
    "'''import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 20  # the number of training iterations\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model Fitting & Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15979496, 0.15774208, 0.15520322, 0.15377738, 0.15525727,\n",
       "       0.16267621, 0.15916723, 0.16098655, 0.15417415, 0.1661801 ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train, sample_weight=None, check_input=True)\n",
    "\n",
    "cross_val_score(clf, X_train, y_train, cv=10)\n",
    "#cross_val_score(clf, iris.data, iris.target, cv=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Boosted Gradient Tree?\n",
    "\n",
    "XG Boost?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "# fit model to training data\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.247271 (0.005717)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.191561 (0.007117)\n",
      "KNN: 0.135135 (0.004719)\n",
      "CART: 0.159052 (0.004795)\n",
      "NB: 0.214292 (0.008149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.141400 (0.003581)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From: https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load dataset\n",
    "#url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "#dataframe = pandas.read_csv(url, names=names)\n",
    "#array = dataframe.values\n",
    "X = X_train\n",
    "Y = y_train\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here to select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Code to load model later...\\n \\n# load the model from disk\\nloaded_model = pickle.load(open(filename, 'rb'))\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code here to pickle best model, so that it can be transfered to Flask + HTML\n",
    "\n",
    "# save the model to disk; move this into the flask app\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "'''\n",
    "# Code to load model later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to predict regex class for\n",
    "a = 'a is the original sentence, b is the replacement sentence!'\n",
    "b = 'a is the original sentence b is the replacement sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to predict regex class for\n",
    "a = 'blah blah1 blah1114324'\n",
    "b = 'blah blah blah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to predict regex class for\n",
    "a = '4324£ ⇐'\n",
    "b = '4324'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because some of my functions from earlier need columns not raw strings:\n",
    "user_inputs = {'sentence': [a], 'end': [b]}\n",
    "user_inputs = pd.DataFrame(user_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Data Transformation\n",
    "Transform the new input data to fit the model input structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in character type counts features\n",
    "\n",
    "a_counts = charCounts(user_inputs['sentence'], 'start_n_')\n",
    "b_counts = charCounts(user_inputs['end'], 'end_n_')\n",
    "\n",
    "input_type_counts = pd.concat([user_inputs, a_counts, b_counts], axis= 1)\n",
    "\n",
    "#string ends feature:\n",
    "string_ends = endChecks(user_inputs, 'sentence','end', regex = False)\n",
    "\n",
    "input_features_combo = pd.merge(input_type_counts, string_ends, on=\"sentence\")\n",
    "\n",
    "X_new = input_features_combo.drop(['sentence','regex','end'], axis=1)\n",
    "\n",
    "#list(input_features_combo.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a function:\n",
    "def cleanInput(a, b):\n",
    "    \"\"\"\n",
    "    Function to perform feature transformation & engineering \n",
    "\n",
    "    @param a: uncleaned sentence\n",
    "    @param b: desired output sentence \n",
    "\n",
    "    @return: feature transformed data\n",
    "    \"\"\"\n",
    "    user_inputs = {'sentence': [a], 'end': [b]}\n",
    "    user_inputs = pd.DataFrame(user_inputs)\n",
    "\n",
    "    #add in character type counts features\n",
    "\n",
    "    a_counts = charCounts(user_inputs['sentence'], 'start_n_')\n",
    "    b_counts = charCounts(user_inputs['end'], 'end_n_')\n",
    "\n",
    "    input_type_counts = pd.concat([user_inputs, a_counts, b_counts], axis= 1)\n",
    "\n",
    "    #string ends feature:\n",
    "    string_ends = endChecks(user_inputs, 'sentence','end', regex = False)\n",
    "\n",
    "    input_features_combo = pd.merge(input_type_counts, string_ends, on=\"sentence\")\n",
    "\n",
    "    X_new = input_features_combo.drop(['sentence','regex','end'], axis=1).values\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., 47.,  0., 47.,  0.,  9., 10.,  0., 47.,  0., 47.,  0.,  9.,\n",
       "        10.,  1.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = cleanInput(a,b)\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Predict Class\n",
    "TODO: For multiple inputs, find median/average highest scored class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now input into the model to predict the class\n",
    "predicted_y = clf.predict(X = X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\.[0]*']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Match that value back to the initial class\n",
    "[regex_dict.get(y) for y in predicted_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicted probabilities of each class\n",
    "#https://datascience.stackexchange.com/questions/22762/understanding-predict-proba-from-multioutputclassifier\n",
    "clf.predict_proba(X_new)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        a: a is the original sentence, b is the replacement sentence!\n",
      "        b: a is the original sentence b is the replacement sentence\n",
      "predicted: a is the original sentence, b is the replacement sentence!\n",
      "    using: \\.[0]*\n"
     ]
    }
   ],
   "source": [
    "r = re.compile([regex_dict.get(y) for y in predicted_y][0])\n",
    "print(\"        a:\", a)\n",
    "print(\"        b:\", b)\n",
    "print(\"predicted:\", re.sub(r,'',a))\n",
    "print(\"    using:\", [regex_dict.get(y) for y in predicted_y][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 code for visualizations and explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeClassifier' object has no attribute 'estimators_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-5df152501cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract single tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeClassifier' object has no attribute 'estimators_'"
     ]
    }
   ],
   "source": [
    "# Visualize the decision tree:\n",
    "# From https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n",
    "\n",
    "# Extract single tree\n",
    "estimator = clf.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = iris.feature_names,\n",
    "                class_names = iris.target_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping Ground for old code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-4ba380d5173c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_regexed_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mchartable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacterTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mstart_by_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_by_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchartable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6209\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6210\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6211\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6213\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5416\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5417\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[0;32m-> 5418\u001b[0;31m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[1;32m   5419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m             b = make_block(\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[0;34m(self, to_concat, placement)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[1;32m    367\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[0;32m--> 368\u001b[0;31m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[1;32m    369\u001b[0m         return self.make_block_same_class(\n\u001b[1;32m    370\u001b[0m             values, placement=placement or slice(0, len(values), 1))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "break original sentence and replacement sentence columns into character counts/presence.\n",
    "'''\n",
    "\n",
    "def characterTable(ref_sentence, sentence, regex = None):\n",
    "    \"\"\"\n",
    "    Break up sentence by character into data frame\n",
    "    \n",
    "    @param ref_sentence: the original sentence, which is used as a reference. Not broken-up\n",
    "    @param sentence: the sentence to break up by character\n",
    "    @param: regex: the regex reference\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(data={\n",
    "        'sentence': ref_sentence,\n",
    "        'regex': regex,\n",
    "        'char': [i for i in sentence]\n",
    "    })\n",
    "    return out\n",
    "\n",
    "start_by_char = pd.DataFrame()\n",
    "\n",
    "for index, row in start_regexed_end.iterrows():\n",
    "    chartable = characterTable(row.sentence, row.sentence, row.regex)\n",
    "    start_by_char = start_by_char.append(chartable)\n",
    "\n",
    "\n",
    "end_by_char = pd.DataFrame()\n",
    "\n",
    "for index, row in start_regexed_end.iterrows():\n",
    "    chartable = characterTable(row.sentence, row.replacement, row.regex)\n",
    "    end_by_char = end_by_char.append(chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use dummies to transpose the character column into their own columns for each character.\n",
    "'''\n",
    "dummies_start = pd.get_dummies(start_by_char, prefix='char_start', columns=['char'])\n",
    "dummies_end = pd.get_dummies(end_by_char, prefix='char_end', columns=['char'])\n",
    "\n",
    "\n",
    "\n",
    "# Define the feature columns\n",
    "r = re.compile('^char\\_start\\_.*')\n",
    "char_start_cols = list(filter(r.match, dummies_start.columns))\n",
    "\n",
    "dummies_start = dummies_start.groupby(['sentence','regex'])[char_start_cols].sum().reset_index()#Now sum up the values\n",
    "\n",
    "r = re.compile('^char\\_end\\_.*')\n",
    "char_end_cols = list(filter(r.match, dummies_end.columns))\n",
    "\n",
    "dummies_end = dummies_end.groupby(['sentence','regex'])[char_end_cols].sum().reset_index()#Now sum up the values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>regex</th>\n",
       "      <th>char_end_</th>\n",
       "      <th>char_end_!</th>\n",
       "      <th>char_end_\"</th>\n",
       "      <th>char_end_#</th>\n",
       "      <th>char_end_$</th>\n",
       "      <th>char_end_%</th>\n",
       "      <th>char_end_&amp;</th>\n",
       "      <th>char_end_'</th>\n",
       "      <th>...</th>\n",
       "      <th>char_end_|</th>\n",
       "      <th>char_end_}</th>\n",
       "      <th>char_end_~</th>\n",
       "      <th>char_end_</th>\n",
       "      <th>char_end_¨</th>\n",
       "      <th>char_end_É</th>\n",
       "      <th>char_end_Ê</th>\n",
       "      <th>char_end_Ð</th>\n",
       "      <th>char_end_Ò</th>\n",
       "      <th>char_end_Õ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[0-9]</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[A-Z]</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[a-z]</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence         regex  \\\n",
       "0  (Aug 22,2011)Plant Status of Fukushima Daiichi...         [0-9]   \n",
       "1  (Aug 22,2011)Plant Status of Fukushima Daiichi...         [A-Z]   \n",
       "2  (Aug 22,2011)Plant Status of Fukushima Daiichi...  [^A-Za-z0-9]   \n",
       "3  (Aug 22,2011)Plant Status of Fukushima Daiichi...         [a-z]   \n",
       "4  (Aug 22,2011)Plant Status of Fukushima Daiichi...           \\d+   \n",
       "\n",
       "   char_end_   char_end_!  char_end_\"  char_end_#  char_end_$  char_end_%  \\\n",
       "0          17           0           0           2           0           0   \n",
       "1          17           0           0           2           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3          17           0           0           2           0           0   \n",
       "4          17           0           0           2           0           0   \n",
       "\n",
       "   char_end_&  char_end_'     ...      char_end_|  char_end_}  char_end_~  \\\n",
       "0           0           0     ...               0           1           0   \n",
       "1           0           0     ...               0           1           0   \n",
       "2           0           0     ...               0           0           0   \n",
       "3           0           0     ...               0           1           0   \n",
       "4           0           0     ...               0           1           0   \n",
       "\n",
       "   char_end_  char_end_¨  char_end_É  char_end_Ê  char_end_Ð  char_end_Ò  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   char_end_Õ  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Combine two dummies tables with start sentence, end sentence and regex\n",
    "\n",
    "May need to rename some columns with strange characters\n",
    "'''\n",
    "\n",
    "# Want to combine the start_regexed_end table and the new dummies_start and dummies_end\n",
    "# Should combine dummies first?\n",
    "# For now keeping dummies table separate from full combo of data, \n",
    "# want to keep set of engineered features separate and combine later.\n",
    "\n",
    "dummies_all = pd.merge(dummies_start, dummies_end, on=[\"sentence\",\"regex\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence',\n",
       " 'regex',\n",
       " 'char_start_ ',\n",
       " 'char_start_!',\n",
       " 'char_start_\"',\n",
       " 'char_start_#',\n",
       " 'char_start_$',\n",
       " 'char_start_%',\n",
       " 'char_start_&',\n",
       " \"char_start_'\",\n",
       " 'char_start_(',\n",
       " 'char_start_)',\n",
       " 'char_start_*',\n",
       " 'char_start_+',\n",
       " 'char_start_,',\n",
       " 'char_start_-',\n",
       " 'char_start_.',\n",
       " 'char_start_/',\n",
       " 'char_start_0',\n",
       " 'char_start_1',\n",
       " 'char_start_2',\n",
       " 'char_start_3',\n",
       " 'char_start_4',\n",
       " 'char_start_5',\n",
       " 'char_start_6',\n",
       " 'char_start_7',\n",
       " 'char_start_8',\n",
       " 'char_start_9',\n",
       " 'char_start_:',\n",
       " 'char_start_;',\n",
       " 'char_start_left_carrot',\n",
       " 'char_start_=',\n",
       " 'char_start_>',\n",
       " 'char_start_?',\n",
       " 'char_start_@',\n",
       " 'char_start_A',\n",
       " 'char_start_B',\n",
       " 'char_start_C',\n",
       " 'char_start_D',\n",
       " 'char_start_E',\n",
       " 'char_start_F',\n",
       " 'char_start_G',\n",
       " 'char_start_H',\n",
       " 'char_start_I',\n",
       " 'char_start_J',\n",
       " 'char_start_K',\n",
       " 'char_start_L',\n",
       " 'char_start_M',\n",
       " 'char_start_N',\n",
       " 'char_start_O',\n",
       " 'char_start_P',\n",
       " 'char_start_Q',\n",
       " 'char_start_R',\n",
       " 'char_start_S',\n",
       " 'char_start_T',\n",
       " 'char_start_U',\n",
       " 'char_start_V',\n",
       " 'char_start_W',\n",
       " 'char_start_X',\n",
       " 'char_start_Y',\n",
       " 'char_start_Z',\n",
       " 'char_start_left_square_bracket',\n",
       " 'char_start_\\\\',\n",
       " 'char_start_right_square_bracket',\n",
       " 'char_start_^',\n",
       " 'char_start__',\n",
       " 'char_start_`',\n",
       " 'char_start_a',\n",
       " 'char_start_b',\n",
       " 'char_start_c',\n",
       " 'char_start_d',\n",
       " 'char_start_e',\n",
       " 'char_start_f',\n",
       " 'char_start_g',\n",
       " 'char_start_h',\n",
       " 'char_start_i',\n",
       " 'char_start_j',\n",
       " 'char_start_k',\n",
       " 'char_start_l',\n",
       " 'char_start_m',\n",
       " 'char_start_n',\n",
       " 'char_start_o',\n",
       " 'char_start_p',\n",
       " 'char_start_q',\n",
       " 'char_start_r',\n",
       " 'char_start_s',\n",
       " 'char_start_t',\n",
       " 'char_start_u',\n",
       " 'char_start_v',\n",
       " 'char_start_w',\n",
       " 'char_start_x',\n",
       " 'char_start_y',\n",
       " 'char_start_z',\n",
       " 'char_start_{',\n",
       " 'char_start_|',\n",
       " 'char_start_}',\n",
       " 'char_start_~',\n",
       " 'char_start_\\x95',\n",
       " 'char_start_¨',\n",
       " 'char_start_É',\n",
       " 'char_start_Ê',\n",
       " 'char_start_Ð',\n",
       " 'char_start_Ò',\n",
       " 'char_start_Õ',\n",
       " 'char_end_ ',\n",
       " 'char_end_!',\n",
       " 'char_end_\"',\n",
       " 'char_end_#',\n",
       " 'char_end_$',\n",
       " 'char_end_%',\n",
       " 'char_end_&',\n",
       " \"char_end_'\",\n",
       " 'char_end_(',\n",
       " 'char_end_)',\n",
       " 'char_end_*',\n",
       " 'char_end_+',\n",
       " 'char_end_,',\n",
       " 'char_end_-',\n",
       " 'char_end_.',\n",
       " 'char_end_/',\n",
       " 'char_end_0',\n",
       " 'char_end_1',\n",
       " 'char_end_2',\n",
       " 'char_end_3',\n",
       " 'char_end_4',\n",
       " 'char_end_5',\n",
       " 'char_end_6',\n",
       " 'char_end_7',\n",
       " 'char_end_8',\n",
       " 'char_end_9',\n",
       " 'char_end_:',\n",
       " 'char_end_;',\n",
       " 'char_end_left_carrot',\n",
       " 'char_end_=',\n",
       " 'char_end_>',\n",
       " 'char_end_?',\n",
       " 'char_end_@',\n",
       " 'char_end_A',\n",
       " 'char_end_B',\n",
       " 'char_end_C',\n",
       " 'char_end_D',\n",
       " 'char_end_E',\n",
       " 'char_end_F',\n",
       " 'char_end_G',\n",
       " 'char_end_H',\n",
       " 'char_end_I',\n",
       " 'char_end_J',\n",
       " 'char_end_K',\n",
       " 'char_end_L',\n",
       " 'char_end_M',\n",
       " 'char_end_N',\n",
       " 'char_end_O',\n",
       " 'char_end_P',\n",
       " 'char_end_Q',\n",
       " 'char_end_R',\n",
       " 'char_end_S',\n",
       " 'char_end_T',\n",
       " 'char_end_U',\n",
       " 'char_end_V',\n",
       " 'char_end_W',\n",
       " 'char_end_X',\n",
       " 'char_end_Y',\n",
       " 'char_end_Z',\n",
       " 'char_end_left_square_bracket',\n",
       " 'char_end_\\\\',\n",
       " 'char_end_right_square_bracket',\n",
       " 'char_end_^',\n",
       " 'char_end__',\n",
       " 'char_end_`',\n",
       " 'char_end_a',\n",
       " 'char_end_b',\n",
       " 'char_end_c',\n",
       " 'char_end_d',\n",
       " 'char_end_e',\n",
       " 'char_end_f',\n",
       " 'char_end_g',\n",
       " 'char_end_h',\n",
       " 'char_end_i',\n",
       " 'char_end_j',\n",
       " 'char_end_k',\n",
       " 'char_end_l',\n",
       " 'char_end_m',\n",
       " 'char_end_n',\n",
       " 'char_end_o',\n",
       " 'char_end_p',\n",
       " 'char_end_q',\n",
       " 'char_end_r',\n",
       " 'char_end_s',\n",
       " 'char_end_t',\n",
       " 'char_end_u',\n",
       " 'char_end_v',\n",
       " 'char_end_w',\n",
       " 'char_end_x',\n",
       " 'char_end_y',\n",
       " 'char_end_z',\n",
       " 'char_end_{',\n",
       " 'char_end_|',\n",
       " 'char_end_}',\n",
       " 'char_end_~',\n",
       " 'char_end_\\x95',\n",
       " 'char_end_¨',\n",
       " 'char_end_É',\n",
       " 'char_end_Ê',\n",
       " 'char_end_Ð',\n",
       " 'char_end_Ò',\n",
       " 'char_end_Õ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Because the algorithms don't like when there are feature names that contain [, ] or <, just rename them\n",
    "Jan 31- right now algorithm doesn't take any feature names at all.\n",
    "'''\n",
    "\n",
    "dummies = dummies_all.copy()\n",
    "\n",
    "# Pickle dummies column names for later use; move this into the flask app\n",
    "dummies_cols = dummies_all.columns\n",
    "pickle.dump(dummies_cols, open('dummies_cols.pickle', 'wb'))\n",
    "\n",
    "# rename offending columns\n",
    "dummies = dummies.rename(columns={\n",
    "    'char_start_[': 'char_start_left_square_bracket',\n",
    "    'char_start_]': 'char_start_right_square_bracket',\n",
    "    'char_start_<': 'char_start_left_carrot',\n",
    "    'char_end_[': 'char_end_left_square_bracket',\n",
    "    'char_end_]': 'char_end_right_square_bracket',\n",
    "    'char_end_<': 'char_end_left_carrot'})\n",
    "\n",
    "# Check it out:\n",
    "list(dummies.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE FOR DUMMIES TABLE FROM USER INPUTS\n",
    "\n",
    "'''\n",
    "OLD- These features no longer work for the training set.\n",
    "Make dummies table for new inputs like I did with the training data\n",
    "'''\n",
    "a_start = characterTable(a,a)\n",
    "b_end = characterTable(a,b)\n",
    "\n",
    "'''\n",
    "Use dummies to transpose the character column into their own columns for each character.\n",
    "'''\n",
    "dummies_a = pd.get_dummies(a_start, prefix='char_start', columns=['char'])\n",
    "dummies_b = pd.get_dummies(b_end, prefix='char_end', columns=['char'])\n",
    "\n",
    "dummies_to_fit = dummies_to_fit.reindex(columns = dummies_cols, fill_value=0)\n",
    "\n",
    "# Define the feature columns\n",
    "r = re.compile('^char\\_start\\_.*')\n",
    "input_char_start_cols = list(filter(r.match, dummies_a.columns))\n",
    "\n",
    "input_dummies_start = dummies_a.groupby(['sentence'])[input_char_start_cols].sum().reset_index()#Now sum up the values\n",
    "\n",
    "r = re.compile('^char\\_end\\_.*')\n",
    "input_char_end_cols = list(filter(r.match, dummies_b.columns))\n",
    "\n",
    "input_dummies_end = dummies_b.groupby(['sentence'])[input_char_end_cols].sum().reset_index()#Now sum up the values\n",
    "\n",
    "input_dummies = pd.merge(input_dummies_start, input_dummies_end, on=\"sentence\")\n",
    "\n",
    "'''\n",
    "Reindex the dummies table to the old columns list so the # of columns will match.\n",
    "This will cause model/site to break? Seems to handle: ⇐\n",
    "If the inputs have characters the model was never trained on this will cause mismatches in the number of columns.\n",
    "'''\n",
    "dummies_to_fit = input_dummies.reindex(columns = dummies_cols, fill_value=0)\n",
    "\n",
    "# rename offending columns\n",
    "dummies_to_fit = dummies_to_fit.rename(columns={\n",
    "    'char_start_[': 'char_start_left_square_bracket',\n",
    "    'char_start_]': 'char_start_right_square_bracket',\n",
    "    'char_start_<': 'char_start_left_carrot',\n",
    "    'char_end_[': 'char_end_left_square_bracket',\n",
    "    'char_end_]': 'char_end_right_square_bracket',\n",
    "    'char_end_<': 'char_end_left_carrot'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# More features?\n",
    "# Character counts by type (number, alphabet, punct etc.)\n",
    "\n",
    "# This is really ugly, something I could do in R very easily:\n",
    "count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "start_counts = start_regexed_end['sentence'].apply(\n",
    "    lambda s: pd.Series(\n",
    "        {'start_n_punct': count(s,set(string.punctuation)),\n",
    "         'start_n_letters': count(s,set(string.ascii_letters)),\n",
    "         'start_n_digits': count(s,set(string.digits)),\n",
    "         'start_n_lower': count(s,set(string.ascii_lowercase)),\n",
    "         'start_n_upper': count(s,set(string.ascii_uppercase)),\n",
    "         'start_n_whitespace': count(s,set(string.whitespace)),\n",
    "         'start_n_words': len(s.split())\n",
    "        }))\n",
    "\n",
    "end_counts = start_regexed_end['replacement'].apply(\n",
    "    lambda s: pd.Series(\n",
    "        {'end_n_punct': count(s,set(string.punctuation)),\n",
    "         'end_n_letters': count(s,set(string.ascii_letters)),\n",
    "         'end_n_digits': count(s,set(string.digits)),\n",
    "         'end_n_lower': count(s,set(string.ascii_lowercase)),\n",
    "         'end_n_upper': count(s,set(string.ascii_uppercase)),\n",
    "         'end_n_whitespace': count(s,set(string.whitespace)),\n",
    "         'end_n_words': len(s.split()),\n",
    "        }))\n",
    "\n",
    "type_counts = pd.concat([start_regexed_end, start_counts, end_counts], axis= 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_regexed_end.sentence[0].split() # Good for words later\n",
    "start_regexed_end.sentence[0][0] #can index by character position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Format differences into dummy variables\n",
    "1. [x] Create dummy variables for each character in sentence\n",
    "2. [ ] Create a second dummy table that captures the number of deletions that occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor each sentence X regex combo:\\n1. find all the diffs, and then match the characters with dummies, and remove that amount from dummies\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For each sentence X regex combo:\n",
    "1. find all the diffs, and then match the characters with dummies, and remove that amount from dummies\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Because the algorithms don't like when there are feature names that contain [, ] or <, just rename them\n",
    "'''\n",
    "\n",
    "dummies = dummies_original.copy()\n",
    "\n",
    "#Pickle dummies column names for later use; move this into the flask app\n",
    "dummies_cols = dummies_original.columns\n",
    "pickle.dump(dummies_cols, open('dummies_cols.pickle', 'wb'))\n",
    "\n",
    "dummies = dummies.rename(columns={'char_[': 'char_left_square_bracket', 'char_]': 'char_right_square_bracket',\n",
    "                       'char_<': 'char_left_carrot'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Because diffs causes multiple training instances, just drop 'diff' for now and then find the column sums\n",
    "\"\"\"\n",
    "dummies = dummies.drop(['diffs'], axis=1) #Drop columns\n",
    "\n",
    "#Define the feature columns\n",
    "r = re.compile('^char\\_.*')\n",
    "char_cols = list(filter(r.match, dummies.columns))\n",
    "\n",
    "dummies = dummies.groupby(['sentence','regex'])[char_cols].sum().reset_index()#Now sum up the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features capturing differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>regex</th>\n",
       "      <th>char</th>\n",
       "      <th>diffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...</td>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[ES]</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>#</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\b\\d+\\b</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\b\\d+\\b</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\b\\d+\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\b\\d+\\b</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Aug 22,2011)Plant Status of Fukushima Daiichi...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>. Renewable Energy Consumption Tops Nuclear fo...</td>\n",
       "      <td>[ES]</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>. Renewable Energy Consumption Tops Nuclear fo...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>. Renewable Energy Consumption Tops Nuclear fo...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>. Renewable Energy Consumption Tops Nuclear fo...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>. Renewable Energy Consumption Tops Nuclear fo...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>. Will liberals now seek to eliminate dangerou...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>. Will liberals now seek to eliminate dangerou...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>. Will liberals now seek to eliminate dangerou...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>. Will liberals now seek to eliminate dangerou...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>:Hello Japan is a nuclear power plant crisis. ...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>{link} The 1940 Olympics, decreased rice consu...</td>\n",
       "      <td>\\b\\d+\\b</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>{link} The 1940 Olympics, decreased rice consu...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>{link} The 1940 Olympics, decreased rice consu...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>{link} The 1940 Olympics, decreased rice consu...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>{link} The 1940 Olympics, decreased rice consu...</td>\n",
       "      <td>\\d+</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@</td>\n",
       "      <td>@</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>@</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>o</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>@\\S+</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>{link} What's happening @mention Fukushima nuc...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>{link} Would you rather have a few dozen wind ...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>{link} Would you rather have a few dozen wind ...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>{link} Would you rather have a few dozen wind ...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>{link} Would you rather have a few dozen wind ...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>{link} YNN/Marist Poll: How New Yorkers feel a...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>{link} YNN/Marist Poll: How New Yorkers feel a...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>{link} YNN/Marist Poll: How New Yorkers feel a...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>{link} YNN/Marist Poll: How New Yorkers feel a...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>{link} YNN/Marist Poll: How New Yorkers feel a...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>{link} health newspaper articles Complete reje...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>{link} health newspaper articles Complete reje...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>{link} health newspaper articles Complete reje...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>{link} health newspaper articles Complete reje...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>{link} health newspaper articles Complete reje...</td>\n",
       "      <td>[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]</td>\n",
       "      <td>}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2634 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "1     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "2     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "3     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "4     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "5     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "6     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "7     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "8     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "9     (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "10    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "11    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "12    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "13    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "14    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "15    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "16    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "17    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "18    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "19    (Aug 22,2011)Plant Status of Fukushima Daiichi...   \n",
       "20    . Renewable Energy Consumption Tops Nuclear fo...   \n",
       "21    . Renewable Energy Consumption Tops Nuclear fo...   \n",
       "22    . Renewable Energy Consumption Tops Nuclear fo...   \n",
       "23    . Renewable Energy Consumption Tops Nuclear fo...   \n",
       "24    . Renewable Energy Consumption Tops Nuclear fo...   \n",
       "25    . Will liberals now seek to eliminate dangerou...   \n",
       "26    . Will liberals now seek to eliminate dangerou...   \n",
       "27    . Will liberals now seek to eliminate dangerou...   \n",
       "28    . Will liberals now seek to eliminate dangerou...   \n",
       "29    :Hello Japan is a nuclear power plant crisis. ...   \n",
       "...                                                 ...   \n",
       "2604  {link} The 1940 Olympics, decreased rice consu...   \n",
       "2605  {link} The 1940 Olympics, decreased rice consu...   \n",
       "2606  {link} The 1940 Olympics, decreased rice consu...   \n",
       "2607  {link} The 1940 Olympics, decreased rice consu...   \n",
       "2608  {link} The 1940 Olympics, decreased rice consu...   \n",
       "2609  {link} What's happening @mention Fukushima nuc...   \n",
       "2610  {link} What's happening @mention Fukushima nuc...   \n",
       "2611  {link} What's happening @mention Fukushima nuc...   \n",
       "2612  {link} What's happening @mention Fukushima nuc...   \n",
       "2613  {link} What's happening @mention Fukushima nuc...   \n",
       "2614  {link} What's happening @mention Fukushima nuc...   \n",
       "2615  {link} What's happening @mention Fukushima nuc...   \n",
       "2616  {link} What's happening @mention Fukushima nuc...   \n",
       "2617  {link} What's happening @mention Fukushima nuc...   \n",
       "2618  {link} What's happening @mention Fukushima nuc...   \n",
       "2619  {link} What's happening @mention Fukushima nuc...   \n",
       "2620  {link} Would you rather have a few dozen wind ...   \n",
       "2621  {link} Would you rather have a few dozen wind ...   \n",
       "2622  {link} Would you rather have a few dozen wind ...   \n",
       "2623  {link} Would you rather have a few dozen wind ...   \n",
       "2624  {link} YNN/Marist Poll: How New Yorkers feel a...   \n",
       "2625  {link} YNN/Marist Poll: How New Yorkers feel a...   \n",
       "2626  {link} YNN/Marist Poll: How New Yorkers feel a...   \n",
       "2627  {link} YNN/Marist Poll: How New Yorkers feel a...   \n",
       "2628  {link} YNN/Marist Poll: How New Yorkers feel a...   \n",
       "2629  {link} health newspaper articles Complete reje...   \n",
       "2630  {link} health newspaper articles Complete reje...   \n",
       "2631  {link} health newspaper articles Complete reje...   \n",
       "2632  {link} health newspaper articles Complete reje...   \n",
       "2633  {link} health newspaper articles Complete reje...   \n",
       "\n",
       "                                                  regex char  diffs  \n",
       "0     [-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...    ,      1  \n",
       "1     [-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...    0      3  \n",
       "2     [-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...    1      2  \n",
       "3     [-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...    2      5  \n",
       "4     [-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?...    3      1  \n",
       "5                                                  [ES]    S      2  \n",
       "6                          [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]          17  \n",
       "7                          [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    #      2  \n",
       "8                          [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    .      1  \n",
       "9                          [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    :      1  \n",
       "10                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "11                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "12                                              \\b\\d+\\b    0      3  \n",
       "13                                              \\b\\d+\\b    1      2  \n",
       "14                                              \\b\\d+\\b    2      5  \n",
       "15                                              \\b\\d+\\b    3      1  \n",
       "16                                                  \\d+    0      3  \n",
       "17                                                  \\d+    1      2  \n",
       "18                                                  \\d+    2      5  \n",
       "19                                                  \\d+    3      1  \n",
       "20                                                 [ES]    E      1  \n",
       "21                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]          12  \n",
       "22                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    .      1  \n",
       "23                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "24                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "25                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]          14  \n",
       "26                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    .      1  \n",
       "27                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "28                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "29                         [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]           8  \n",
       "...                                                 ...  ...    ...  \n",
       "2604                                            \\b\\d+\\b    9      1  \n",
       "2605                                                \\d+    0      1  \n",
       "2606                                                \\d+    1      1  \n",
       "2607                                                \\d+    4      1  \n",
       "2608                                                \\d+    9      1  \n",
       "2609                                                  @    @      1  \n",
       "2610                                               @\\S+    @      1  \n",
       "2611                                               @\\S+    e      1  \n",
       "2612                                               @\\S+    i      1  \n",
       "2613                                               @\\S+    m      1  \n",
       "2614                                               @\\S+    n      2  \n",
       "2615                                               @\\S+    o      1  \n",
       "2616                                               @\\S+    t      1  \n",
       "2617                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]           7  \n",
       "2618                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "2619                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "2620                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]          20  \n",
       "2621                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    .      1  \n",
       "2622                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "2623                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "2624                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]          10  \n",
       "2625                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    /      1  \n",
       "2626                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    :      1  \n",
       "2627                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "2628                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "2629                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]          15  \n",
       "2630                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    -      1  \n",
       "2631                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    .      3  \n",
       "2632                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    {      1  \n",
       "2633                       [^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]    }      1  \n",
       "\n",
       "[2634 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dummies for the differences when they occur\n",
    "'''\n",
    "1. If diffs == '-', then create/find column with char\n",
    "'''\n",
    "sent.loc[sent.diffs=='-'].groupby(['sentence','regex','char'])['diffs'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out shit for character type counting etc.\n",
    "\n",
    "'start_n_': count(s,set(string.)),\n",
    "\n",
    "def mo(input1):\n",
    "    return input1.count(\"!\")+2\n",
    "\n",
    "def mo2(input1):\n",
    "    return input1.count(\"!\") + 22\n",
    "\n",
    "\n",
    "aux = start_regexed_end['sentence'].apply(lambda x: pd.Series({'feature1':mo(x), 'feature2':mo2(x)}))\n",
    "\n",
    "\n",
    "modf = pd.concat([start_regexed_end, aux], axis= 1)\n",
    "modf.head()\n",
    "\n",
    "\n",
    "# make new df to store these features in\n",
    "\n",
    "counts = pd.DataFrame(columns = ['sentence','regex', # for indexing\n",
    "                                 'start_n_alphabet', # alphabetical characters\n",
    "                                 'end_n_alphabet',\n",
    "                                 'start_n_numeric', # numeric characters\n",
    "                                 'end_end_numeric',\n",
    "                                 'start_n_punct', # punctuations\n",
    "                                 'end_n_punct'])\n",
    "\n",
    "for index, row in start_regexed_end.iterrows():\n",
    "    \n",
    "    print(len([c for c in row.sentence if c.isdigit()])) # start numbers\n",
    "    print(len([c for c in row.replacement if c.isdigit()])) # end numbers\n",
    "\n",
    "    \n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "count(s,set(string.punctuation))\n",
    "count(s,set(string.ascii_letters))\n",
    "count(s,set(string.digits))\n",
    "count(s,set(string.ascii_lowercase))\n",
    "count(s,set(string.ascii_uppercase))\n",
    "count(s,set(string.whitespace))\n",
    "\n",
    "\n",
    "\n",
    "# words split on whitespace?\n",
    "len(s.split())\n",
    "\n",
    "# count white space\n",
    "len(s) - len(s.strip())\n",
    "\n",
    "string.ascii_letters\n",
    "\n",
    "string.numbers\n",
    "\n",
    "s = 'abcd!!!'\n",
    "\n",
    "count(s,set(string.punctuation))       \n",
    "    \n",
    "import string\n",
    "a = \"I'm not gonna post my homework as question on OS again, I'm not gonna...\"\n",
    "\n",
    "count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "a_chars =  count(a, string.ascii_letters)\n",
    "a_punct = count(a, string.punctuation)\n",
    "\n",
    "    \n",
    "    \n",
    "start_regexed_end.apply(lambda sentence: len([c for c in sentence if c.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for index, row in start_regexed_end.iterrows():\n",
    "    print(len([c for c in row.sentence if c.isdigit()]))\n",
    "\n",
    "\n",
    "len([c for c in s if c.isdigit()])\n",
    "\n",
    "\n",
    "sum(c.isdigit() for c in s) \n",
    "\n",
    "# punctuations\n",
    "\n",
    "\n",
    "#start_regexed_end.sentence[0].split() # Good for words later\n",
    "start_regexed_end.sentence[0][0] #can index by character position\n",
    "\n",
    "len(start_regexed_end.sentence[0])\n",
    "\n",
    "[c for c in start_regexed_end.sentence[0] if c.isdigit()]\n",
    "\n",
    "\n",
    "len([c for c in start_regexed_end.sentence[10] if c.isdigit()])\n",
    "\n",
    "\n",
    "def characterTable(ref_sentence, sentence, regex = None):\n",
    "    \"\"\"\n",
    "    Break up sentence by character into data frame\n",
    "    \n",
    "    @param ref_sentence: the original sentence, which is used as a reference. Not broken-up\n",
    "    @param sentence: the sentence to break up by character\n",
    "    @param: regex: the regex reference\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(data={\n",
    "        'sentence': ref_sentence,\n",
    "        'regex': regex,\n",
    "        'char': [i for i in sentence]\n",
    "    })\n",
    "    return out\n",
    "\n",
    "start_by_char = pd.DataFrame()\n",
    "\n",
    "for index, row in start_regexed_end.iterrows():\n",
    "    chartable = characterTable(row.sentence, row.sentence, row.regex)\n",
    "    start_by_char = start_by_char.append(chartable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OLD CELL!\n",
    "\n",
    "Cool, created this data frame with some dummy data! \n",
    "But we should create some actual features that might important, e.g., difference\n",
    "'''\n",
    "def calcDiffs(a,b, regex = None):\n",
    "    d = difflib.Differ()\n",
    "    diff = d.compare(a,b)\n",
    "    tmp = list(diff)\n",
    "    out = pd.DataFrame(data={\n",
    "        'sentence': a,\n",
    "        'regex': regex,\n",
    "        #'diffs': [i[0] for i in tmp],\n",
    "        'char': [i[2] for i in tmp]\n",
    "    })\n",
    "    \n",
    "    return out\n",
    "\n",
    "sent = pd.DataFrame()\n",
    "for index, row in dat.iterrows():\n",
    "    a = row.sentence\n",
    "    b = row.replacement\n",
    "    \n",
    "    out = calcDiffs(a,b,row.regex)\n",
    "    \n",
    "    sent = sent.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "Using the dummies table from the trained data set, reindex the new dummies to fit the same standard\n",
    "https://stackoverflow.com/questions/28465633/easy-way-to-apply-transformation-from-pandas-get-dummies-to-new-data\n",
    "'''\n",
    "dummies_to_fit = pd.get_dummies(out, columns=['char'])\n",
    "\n",
    "#dummies_to_fit = dummies_to_fit.reindex(columns = dummies_cols, fill_value=0)\n",
    "# need to reindex on new feature set\n",
    "\n",
    "#Rename the columns\n",
    "# need to check the order this happens\n",
    "dummies_to_fit = dummies_to_fit.rename(\n",
    "    columns={'char_[': 'char_left_square_bracket',\n",
    "             'char_]': 'char_right_square_bracket',\n",
    "             'char_<': 'char_left_carrot'})\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "X_new = dummies_to_fit.drop(['sentence', 'regex',], axis=1) #Remove everything but the dummy variables\n",
    "\n",
    "#For whatever reason it's creating a separate matrix for each one. Add them all together instead\n",
    "X_new = X_new.sum().values\n",
    "\n",
    "#Reshape data for single sample\n",
    "X_new = X_new.reshape(1, -1)\n",
    "\n",
    "#add in character type counts features\n",
    "\n",
    "a_counts = charCounts(user_inputs['start'], 'start_n_')\n",
    "b_counts = charCounts(user_inputs['end'], 'end_n_')\n",
    "\n",
    "input_type_counts = pd.concat([user_inputs, a_counts, b_counts], axis= 1)\n",
    "\n",
    "input_features_combo = pd.merge(input_type_counts, dummies_to_fit, on=\"sentence\")\n",
    "\n",
    "list(features_combo.columns.values)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
